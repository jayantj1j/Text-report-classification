{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "stopwords=np.loadtxt('https://a5a5687e-a-62cb3a1a-s-sites.googlegroups.com/site/kevinbouge/stopwords-lists/stopwords_en.txt?attachauth=ANoY7crcABAfBg6e5ZA_6lZ79_r4NuPw2j4KdOLBq9sySv5C18RCykRhnGoN-pG2hfvk4B2jqk2lDxK5Al2FVEfUXu4dbWV-7HDgYA7JrFSMyvIu7r9BNi1uAPe_wiuEFCLptEBoK3D3NOBv-un9McQ2nVZ2RsPF1EQmwLN3n7wS6OlSWMUeI6xaXvU_Tvsqwi5DQOOaIx6zw11Ry6wptN3n3aUuF8Tw8wJYhWoPQCpKTZ0eukRtmvk%3D&attredirects=0&d=1',dtype= str , delimiter=',').tolist()\n",
    "allwords=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob  \n",
    "import errno\n",
    "x=[]\n",
    "\n",
    "total_filefolder_eaachtype=1 #Each type folder has 800 files of same type\n",
    "total_classes=20 \n",
    "\n",
    "\n",
    "for name in glob.glob(\"C:\\\\Users\\\\Jayant\\\\Desktop\\\\datasets\\\\textclassify\\\\train\\\\*\"):\n",
    "         x.append(name[52:])\n",
    "        \n",
    "        \n",
    "     \n",
    "for i in range(len(x)):\n",
    "    \n",
    "        path = \"C:\\\\Users\\\\Jayant\\\\Desktop\\\\datasets\\\\textclassify\\\\train\\\\\"+x[i]+\"\\\\*\" #note C:\n",
    "        files = glob.glob(path)\n",
    "        \n",
    "        with open(\"C:\\\\Users\\\\Jayant\\Desktop\\\\datasets\\\\textclassify\\\\\"+\"file\"+str(i), 'w') as outfile:\n",
    "            for name in files:\n",
    "                with open(name) as infile:\n",
    "                         for line in infile:\n",
    "                            pass\n",
    "                           #     outfile.write(line)\n",
    "            \n",
    "\n",
    "            \n",
    "                \n",
    "                                \n",
    "                                \n",
    "#          with open(name) as f:\n",
    "#             for line in f:\n",
    "#                 print(line.split())\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"C:\\\\Users\\\\Jayant\\\\Desktop\\\\datasets\\\\textclassify\\\\trainconcat\"\n",
    "files = os.listdir(path)\n",
    "i = 0\n",
    "while i<20:\n",
    "    os.rename(os.path.join(path,files[i]), os.path.join(path,x[i]))\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words):\n",
    "    words = [word for word in words if not word in stopwords]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(words):\n",
    "    words = [word for word in words if len(word) > 2]\n",
    "    words = [word for word in words if not word.isdigit()]\n",
    "    words = [str for str in words if str]\n",
    "            \n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(line):\n",
    "    words = line[0:len(line)-1].strip().split(\" \")\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    words1 = preprocess(words)\n",
    "    words2 = remove_stopwords(words1)\n",
    "    \n",
    "    return words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr= glob.glob(\"C:\\\\Users\\\\Jayant\\\\Desktop\\\\datasets\\\\textclassify\\\\train\\\\sci.crypt\\\\8output.txt\")\n",
    "myffdict={}\n",
    "\n",
    "    \n",
    " \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "    \n",
    "for i in range(len(x)):\n",
    "        c=0\n",
    "        path = \"C:\\\\Users\\\\Jayant\\\\Desktop\\\\datasets\\\\textclassify\\\\test\\\\\"+x[i]+\"\\\\*\" #note C:\n",
    "        files = glob.glob(path)\n",
    "        newpath = r'C:\\\\Users\\\\Jayant\\\\Desktop\\\\datasets\\\\textclassify\\\\'+x[i] \n",
    "        os.makedirs(newpath)\n",
    "\n",
    "        \n",
    "        for name in files:\n",
    "            c+=1\n",
    "            with open(name,'rb') as infile,open(newpath+'\\\\'+str(c)+'output.txt', 'wb') as outfile:\n",
    "        \n",
    "                  for line in infile:\n",
    "                        if not line.strip(): continue  # skip the empty line\n",
    "                       # outfile.write(line)\n",
    "   \n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for t in range(len(x)):\n",
    "    \n",
    "    loc=\"C:\\\\Users\\\\Jayant\\\\Desktop\\\\datasets\\\\textclassify\\\\train\\\\\"+x[t]+\"\\\\*\"\n",
    "    for k in glob.glob(loc):\n",
    "        print(k)\n",
    "        lines = open(k, errors='ignore').readlines()\n",
    "        \n",
    "        del lines[:13]\n",
    "        \n",
    "            \n",
    "        lines=\"\".join(lines)\n",
    "        \n",
    "        allwords.extend(tokenize_sentence(lines))\n",
    "        \n",
    "        \n",
    "iword=compile(allwords)\n",
    "        \n",
    "iword        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "word_count={}\n",
    "def compile(t):   \n",
    "    \n",
    "    for i in t:\n",
    "        for k in [x.lower() for x in re.findall(r'\\w+',i.strip())]:# This statement first converts the into list of words only(excluding all characters) and then \"k\" iterates over that list\n",
    "             ##condition for the word being checked  does not lie in stopword dataset. \n",
    "            if k not in word_count.keys(): # if the above condition is true then start initializing & counting of words in dictionary\n",
    "                word_count[k]=1 \n",
    "            else:\n",
    "                word_count[k]+=1\n",
    "    return word_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for keys in list(iword.keys()):\n",
    "    \n",
    "    if keys in stopwords or len(keys)<=2 or keys.isdigit() or type(keys)!=str:\n",
    "        del iword[keys]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortlisttuple = sorted(iword.items(), key=lambda kv: kv[1],reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fvocab={}\n",
    "for i,j in sortlisttuple:\n",
    "    if(j>=100):\n",
    "        fvocab[i]=j\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fvocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result={}\n",
    "\n",
    "import mmap\n",
    "for i in range(len(x)):\n",
    "    \n",
    "    path = \"C:\\\\Users\\\\Jayant\\\\Desktop\\\\datasets\\\\textclassify\\\\test\\\\\"+x[i]+\"\\\\*\" #note C:\n",
    "    files = glob.glob(path)\n",
    "    result[x[i]]={}\n",
    "    result[x[i]][\"total_count\"] = len(files)\n",
    "        \n",
    "    for name in files:\n",
    "               \n",
    "            with open(name,'rb') as file:\n",
    "                   s= mmap.mmap(file.fileno(), 0, access=mmap.ACCESS_READ)\n",
    "                   for j in fvocab.keys():\n",
    "                            if s.find(j.encode()) != -1:\n",
    "                                if j in result[x[i]].keys(): # if the above condition is true then start initializing & counting of words in dictionary\n",
    "                                    result[x[i]][j]+=1 \n",
    "                                else:\n",
    "                                    result[x[i]][j]=1\n",
    "                    \n",
    "                       \n",
    "\n",
    "                        \n",
    "\n",
    "                        \n",
    "                        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "    \n",
    "    \n",
    "        result[x[i]]['#cattotal']=sum(result[x[i]].values())-800\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "    c=0\n",
    "    path = \"C:\\\\Users\\\\Jayant\\\\Desktop\\\\datasets\\\\textclassify\\\\test\\\\\"+x[i]+\"\\\\*\" #note C:\n",
    "    f = glob.glob(path)\n",
    "    f=f[0:10]\n",
    "    newpath = r'C:\\\\Users\\\\Jayant\\\\Desktop\\\\datasets\\\\textclassify\\\\'+x[i] \n",
    "    os.makedirs(newpath)\n",
    "    for name in f:\n",
    "            c+=1\n",
    "            with open(name,'rb') as infile,open(newpath+'\\\\'+str(c)+'output.txt', 'wb') as outfile:\n",
    "\n",
    "                          for line in infile:\n",
    "                                if not line.strip(): continue  # skip the empty line\n",
    "                                #outfile.write(line)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yact=[];ypred=[];i=0\n",
    "for i in range(20):\n",
    "    \n",
    "    path = \"C:\\\\Users\\\\Jayant\\\\Desktop\\\\datasets\\\\textclassify\\\\test2\\\\\"+x[i]+\"\\\\*\" #note C:\n",
    "    files = glob.glob(path)\n",
    "    \n",
    "        \n",
    "    for name in files:\n",
    "        \n",
    "        yact.append(x[i])\n",
    "\n",
    "        comp=[];prob=[]\n",
    "        lines = open(name, errors='ignore').readlines()\n",
    "\n",
    "        del lines[:10]\n",
    "        k=0\n",
    "        #print(lines)\n",
    "        \n",
    "                                \n",
    "        \n",
    "        for u in range(len(x)):\n",
    "            rat=0\n",
    "            for pp in result[x[u]].keys():\n",
    "                k=0\n",
    "                   \n",
    "                for line in lines:\n",
    "                    \n",
    "                    words = line.split()\n",
    "                    for s in words:\n",
    "                        if(s==pp):\n",
    "                            k=k+1\n",
    "                                \n",
    "                \n",
    "\n",
    "                rat=rat+k*(np.log(result[x[u]][pp]+1)-np.log(result[x[u]]['#cattotal']+5623))\n",
    "            \n",
    "                \n",
    "\n",
    "                    \n",
    "            prob.append(rat)\n",
    "        print(prob)\n",
    "            \n",
    "                \n",
    "        ans=prob.index(max(prob))          \n",
    "        ypred.append(x[ans])            \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            \n",
    "            \n",
    "\n",
    "                    \n",
    "               \n",
    "            \n",
    "   \n",
    "    \n",
    "        \n",
    "                       \n",
    "\n",
    "                        \n",
    "\n",
    "                        \n",
    "                        \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "print(confusion_matrix(yact, ypred))\n",
    "print(classification_report(yact, ypred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
